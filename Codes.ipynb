{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.porter import * \n",
    "\n",
    "# Feature Engineering\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the training data from Sentiment140\n",
    "zipurl = 'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   score          id                          date     query             user  \\\n",
      "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
      "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
      "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
      "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
      "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
      "\n",
      "                                               tweet  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('training.1600000.processed.noemoticon.csv', \n",
    "                    header = None,\n",
    "                    names = ['score', 'id', 'date', 'query', 'user', 'tweet'],\n",
    "                    encoding='latin-1')\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          score                                              tweet\n",
      "0            0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1            0  is upset that he can't update his Facebook by ...\n",
      "2            0  @Kenichan I dived many times for the ball. Man...\n",
      "3            0    my whole body feels itchy and like its on fire \n",
      "4            0  @nationwideclass no, it's not behaving at all....\n",
      "...        ...                                                ...\n",
      "1599995      4  Just woke up. Having no school is the best fee...\n",
      "1599996      4  TheWDB.com - Very cool to hear old Walt interv...\n",
      "1599997      4  Are you ready for your MoJo Makeover? Ask me f...\n",
      "1599998      4  Happy 38th Birthday to my boo of alll time!!! ...\n",
      "1599999      4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
      "\n",
      "[1600000 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "# extract the information that we want\n",
    "data_train = train[['score', 'tweet']]\n",
    "print(data_train.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "sentencizer = nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "def tweet_processing(comment):\n",
    "    \n",
    "    modComm = comment\n",
    "    \n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    modComm = re.sub(r'\\&\\w*;', '', modComm)\n",
    "    # Convert @username to AT_USER\n",
    "    modComm = re.sub('@[^\\s]+','',modComm)\n",
    "    # Remove tickers\n",
    "    modComm = re.sub(r'\\$\\w*', '', modComm)\n",
    "    # To lowercase\n",
    "    modComm = modComm.lower()\n",
    "    # Remove hyperlinks\n",
    "    modComm = re.sub(r'https?:\\/\\/.*\\/\\w*', '', modComm)\n",
    "    # Remove hashtags\n",
    "    modComm = re.sub(r'#\\w*', '', modComm)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    modComm = re.sub(r'\\s\\s+', ' ', modComm)\n",
    "    # Remove single space remaining at the front of the tweet.\n",
    "    modComm = modComm.lstrip(' ') \n",
    "    \n",
    "    return modComm\n",
    "\n",
    "def text_processing(tweet):\n",
    "    \n",
    "     # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in list(tweet) if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.lower().split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "# Lexicon normalisation with Stemming \n",
    "def stemming(tokens):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Replace words for its root based on orter Stemmer rule.\n",
    "    2. Returns normalised text\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    x = [stemmer.stem(w) for w in tokens]\n",
    "    \n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = []\n",
    "for i in range(0, len(data_train), 1):\n",
    "    comment = stemming(text_processing(tweet_processing(data_train['tweet'][i])))\n",
    "    mat.append([data_train['score'][i], comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "2\n",
      "2\n",
      "[[0, 'awww that bummer shoulda got david carr third day'], [0, 'upset cant updat facebook text might cri result school today also blah'], [0, 'dive mani time ball manag save 50 rest go bound'], [0, 'whole bodi feel itchi like fire'], [0, 'behav im mad cant see'], [0, 'whole crew'], [0, 'need hug'], [0, 'hey long time see ye rain bit bit lol im fine thank how'], [0, 'nope didnt'], [0, 'que muera']]\n"
     ]
    }
   ],
   "source": [
    "print(len(mat))\n",
    "print(len(mat[0]))\n",
    "print(len(mat[1]))\n",
    "print(mat[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = pd.DataFrame(mat, columns=['score', 'tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.to_csv('cleaned_train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.dropna(subset=['tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def polarity_scores_all(tweet):\n",
    "  '''\n",
    "  Takes string of text to:\n",
    "  1. Gets sentiment metrics\n",
    "  2. Returns negative, neutral, positive \n",
    "  and compound scores as lists.\n",
    "  '''\n",
    "  neg, neu, pos, compound = [], [], [], []\n",
    "  analyser = SentimentIntensityAnalyzer()\n",
    "  \n",
    "  for text in tweet:\n",
    "    dict_ = analyser.polarity_scores(text)\n",
    "    neg.append(dict_['neg'])\n",
    "    neu.append(dict_['neu'])\n",
    "    pos.append(dict_['pos'])\n",
    "    compound.append(dict_['compound'])\n",
    "  \n",
    "  return neg, neu, pos, compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = polarity_scores_all(mat.tokens.values)\n",
    "mat['neg_scores'] = all_scores[0]\n",
    "mat['neu_scores'] = all_scores[1]\n",
    "mat['pos_scores'] = all_scores[2]\n",
    "mat['compound_scores'] = all_scores[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg_scores</th>\n",
       "      <th>neu_scores</th>\n",
       "      <th>pos_scores</th>\n",
       "      <th>compound_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww that bummer shoulda got david carr third day</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dive mani time ball manag save 50 rest go bound</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                             tokens  neg_scores  \\\n",
       "0      0  awww that bummer shoulda got david carr third day       0.245   \n",
       "1      0  upset cant updat facebook text might cri resul...       0.286   \n",
       "2      0    dive mani time ball manag save 50 rest go bound       0.000   \n",
       "3      0                    whole bodi feel itchi like fire       0.270   \n",
       "\n",
       "   neu_scores  pos_scores  compound_scores  \n",
       "0       0.755       0.000          -0.3818  \n",
       "1       0.714       0.000          -0.4588  \n",
       "2       0.738       0.262           0.4939  \n",
       "3       0.449       0.281           0.0258  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tony_niu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To transform pos tags to readable tags\n",
    "pos_family = {  \n",
    "    'NOUN' : ['NN','NNS','NNP'], # Removed 'NNPS'\n",
    "    'PRON' : ['PRP','PRP$','WP','WP$'],\n",
    "    'VERB' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'ADJ' :  ['JJ','JJR','JJS'],\n",
    "    'ADV' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "def count_pos_tag(tweets):\n",
    "    '''\n",
    "    Takes string of text to:\n",
    "    1. Processes text and attaches POS tags\n",
    "    2. Input the dictionary of POS tags into a Counter.\n",
    "    3. Returns list of POS tags with occurrence number.\n",
    "    '''\n",
    "    total_count = []\n",
    "    for s in tweets:\n",
    "        partial_count = {}\n",
    "        s = s.split()\n",
    "        count_pos = Counter(dict(nltk.pos_tag(s)).values())\n",
    "        \n",
    "        for item, value in count_pos.items():\n",
    "            partial_count[item] = partial_count.get(item, 0) + 1\n",
    "            \n",
    "        total_count.append(partial_count)\n",
    "        \n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NN', 'VBD', 'JJ', 'MD', 'VB', 'RB', 'NNS', 'VBP', 'CD', 'WRB', 'VBZ',\n",
       "       'VBN', 'UH', 'CC', 'DT', 'PRP', 'JJR', 'PRP$', 'RP', 'JJS', 'NNP',\n",
       "       'VBG', 'EX', 'RBR', 'RBS', 'FW', 'TO', 'WP', 'WDT', 'SYM', 'PDT', 'WP$',\n",
       "       '''', 'LS', 'POS', '``', 'NNPS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve POS tags with occurrence \n",
    "total_count = count_pos_tag(mat.tokens.values)\n",
    "\n",
    "# As dataframe \n",
    "pos_df = pd.DataFrame(total_count)\n",
    "\n",
    "# Remove unwanted characters\n",
    "pos_df = pos_df.drop(['$', 'IN'], axis = 1) #drop '$' if needed\n",
    "\n",
    "# Inspection\n",
    "pos_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change tags to readable tags\n",
    "\n",
    "pos_df['NOUN'] = pos_df[pos_family['NOUN']].sum(axis=1)\n",
    "pos_df['PRON'] = pos_df[pos_family['PRON']].sum(axis=1)\n",
    "pos_df['VERB'] = pos_df[pos_family['VERB']].sum(axis=1)\n",
    "pos_df['ADJ'] = pos_df[pos_family['ADJ']].sum(axis=1)\n",
    "pos_df['ADV'] = pos_df[pos_family['ADV']].sum(axis=1)\n",
    "\n",
    "pos_df = pos_df[['NOUN', 'PRON', 'VERB', 'ADJ', 'ADV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to end of original data set as new features \n",
    "mat = pd.concat([mat, pos_df], axis = 1)\n",
    "\n",
    "# Deal with NaN\n",
    "mat = mat.fillna(value=0.0)\n",
    "\n",
    "#train = train.fillna(value=0.0)\n",
    "mat.shape\n",
    "\n",
    "# Remove duplicates \n",
    "mat.drop_duplicates(subset=['tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1476969 entries, 0 to 1599998\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   score            1476969 non-null  int64  \n",
      " 1   tokens           1476969 non-null  object \n",
      " 2   neg_scores       1476969 non-null  float64\n",
      " 3   neu_scores       1476969 non-null  float64\n",
      " 4   pos_scores       1476969 non-null  float64\n",
      " 5   compound_scores  1476969 non-null  float64\n",
      " 6   NOUN             1476969 non-null  float64\n",
      " 7   PRON             1476969 non-null  float64\n",
      " 8   VERB             1476969 non-null  float64\n",
      " 9   ADJ              1476969 non-null  float64\n",
      " 10  ADV              1476969 non-null  float64\n",
      "dtypes: float64(9), int64(1), object(1)\n",
      "memory usage: 135.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg_scores</th>\n",
       "      <th>neu_scores</th>\n",
       "      <th>pos_scores</th>\n",
       "      <th>compound_scores</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww that bummer shoulda got david carr third day</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dive mani time ball manag save 50 rest go bound</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>behav im mad cant see</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                             tokens  neg_scores  \\\n",
       "0      0  awww that bummer shoulda got david carr third day       0.245   \n",
       "1      0  upset cant updat facebook text might cri resul...       0.286   \n",
       "2      0    dive mani time ball manag save 50 rest go bound       0.000   \n",
       "3      0                    whole bodi feel itchi like fire       0.270   \n",
       "4      0                              behav im mad cant see       0.444   \n",
       "\n",
       "   neu_scores  pos_scores  compound_scores  NOUN  PRON  VERB  ADJ  ADV  \n",
       "0       0.755       0.000          -0.3818   1.0   0.0   1.0  1.0  0.0  \n",
       "1       0.714       0.000          -0.4588   1.0   0.0   2.0  1.0  1.0  \n",
       "2       0.738       0.262           0.4939   2.0   0.0   1.0  1.0  0.0  \n",
       "3       0.449       0.281           0.0258   2.0   0.0   1.0  1.0  0.0  \n",
       "4       0.556       0.000          -0.4939   1.0   0.0   0.0  1.0  0.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check new features\n",
    "mat.info()\n",
    "\n",
    "mat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.to_csv('feat_eng_train_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'training.1600000.processed.noemoticon.csv'\n",
    "os.remove(file_path)\n",
    "file_path = 'testdata.manual.2009.06.14.csv'\n",
    "os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
