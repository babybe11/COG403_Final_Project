{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg_scores</th>\n",
       "      <th>neu_scores</th>\n",
       "      <th>pos_scores</th>\n",
       "      <th>compound_scores</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww that bummer shoulda got david carr third day</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset cant updat facebook text might cri resul...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dive mani time ball manag save 50 rest go bound</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                             tokens  neg_scores  \\\n",
       "0      0  awww that bummer shoulda got david carr third day       0.245   \n",
       "1      0  upset cant updat facebook text might cri resul...       0.286   \n",
       "2      0    dive mani time ball manag save 50 rest go bound       0.000   \n",
       "\n",
       "   neu_scores  pos_scores  compound_scores  NOUN  PRON  VERB  ADJ  ADV  \n",
       "0       0.755       0.000          -0.3818   1.0   0.0   1.0  1.0  0.0  \n",
       "1       0.714       0.000          -0.4588   1.0   0.0   2.0  1.0  1.0  \n",
       "2       0.738       0.262           0.4939   2.0   0.0   1.0  1.0  0.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data TODO: add file name\n",
    "training = pd.read_csv('feat_eng_train_data.csv')\n",
    "\n",
    "# remove rows with none values\n",
    "training = training.dropna(0, 'any')\n",
    "\n",
    "# Features TODO: correct feature names\n",
    "features = ['tokens', 'neu_scores', 'neg_scores', 'compound_scores', 'pos_scores']\n",
    "label = ['score']\n",
    "\n",
    "# Saving features and label data in X and y for train-test split\n",
    "X = training[[col for col in training.columns if col in features]]\n",
    "y = training[label]\n",
    "\n",
    "# splitting data into training and validation set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "training.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from gracecarrillo\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "# Pipeline to convert tweets to a matrix of TF-IDF features.\n",
    "tfidf = Pipeline([\n",
    "                ('selector', TextSelector(key='tokens')),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])\n",
    "\n",
    "# Pipeline to convert tweets to a matrix of token counts\n",
    "countvect = Pipeline([\n",
    "                ('selector', TextSelector(key='tokens')),\n",
    "                ('countvect', CountVectorizer())\n",
    "            ])\n",
    "\n",
    "# Applying tfidf anf countvec to features\n",
    "neu_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='neu_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])\n",
    "neg_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='neg_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])\n",
    "pos_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='pos_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])\n",
    "\n",
    "compound_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='compound_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining different sets of text processors\n",
    "def features_union(textProcessor):\n",
    "    return FeatureUnion([('tokens', textProcessor),\n",
    "                      ('neu_scores', neu_scores),\n",
    "                      ('neg_scores', neg_scores),\n",
    "                      ('pos_scores', pos_scores),\n",
    "                      ('compound_scores', compound_scores)])\n",
    "# Normalise labels\n",
    "le = LabelEncoder().fit(y_train.ravel())\n",
    "\n",
    "y_train = le.transform(y_train.ravel())\n",
    "y_test = le.transform(y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7624393183341571"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive-Bayes Classifier\n",
    "\n",
    "# instantiate classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# combine features\n",
    "features_count = features_union(countvect)\n",
    "\n",
    "# define pipeline object \n",
    "nb_pipeline = Pipeline([('features', features_count),\n",
    "                       ('nb', clf)])\n",
    "\n",
    "# Fit classifier\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# score\n",
    "nb_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7667725139982532"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "\n",
    "# instantiate classifier\n",
    "svm = LinearSVC()\n",
    "\n",
    "#  combine features\n",
    "features_tfidf = features_union(tfidf)\n",
    "\n",
    "# define pipeline object\n",
    "svm_pipeline = Pipeline([('features', features_tfidf),\n",
    "                       ('svm', svm)])\n",
    "\n",
    "# Fit classifier\n",
    "svm_pipeline.fit(X_train, y_train.ravel())\n",
    "\n",
    "# score\n",
    "svm_pipeline.score(X_test, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# cross valiadation for Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation for Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.4 MB 637 kB/s eta 0:00:011   |█▉                              | 22.1 MB 9.7 MB/s eta 0:00:39     |██                              | 24.4 MB 9.7 MB/s eta 0:00:39     |███████████████                 | 183.8 MB 27.4 MB/s eta 0:00:08     |█████████████████████▉          | 268.6 MB 19.4 MB/s eta 0:00:07     |███████████████████████▎        | 287.3 MB 28.2 MB/s eta 0:00:04     |████████████████████████        | 295.6 MB 28.2 MB/s eta 0:00:04     |████████████████████████▏       | 298.3 MB 28.2 MB/s eta 0:00:04     |██████████████████████████▋     | 328.1 MB 20.1 MB/s eta 0:00:04"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.8/site-packages (2.4.3)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.8/site-packages (from keras) (3.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.8/site-packages (from keras) (1.19.4)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.8/site-packages (from keras) (1.5.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from keras) (5.3.1)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.8/site-packages (from keras) (1.19.4)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.8/site-packages (from keras) (1.19.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-c3adbfdf93fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7e961d7daa86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create the tokenizer (tweets have been preprocessed so no need for filters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fit the tokenizer on tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameter indicating the number of words\n",
    "nb_words = 10000  \n",
    "\n",
    "# create the tokenizer (tweets have been preprocessed so no need for filters)\n",
    "tk = Tokenizer(num_words=nb_words)\n",
    "\n",
    "# fit the tokenizer on tweets\n",
    "tk.fit_on_texts(training.tokens)\n",
    "\n",
    "# integer encode tweets\n",
    "tweets_seq = tk.texts_to_sequences(training.tokens)\n",
    "\n",
    "# TODO need to update based, uncomment line below to see what the max is\n",
    "# print(training['word count'].describe())\n",
    "max_len = 39\n",
    "\n",
    "# Convert sequences into 2-D Numpy arrays\n",
    "features = pad_sequences(tweets_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"label\"] = training[\"label\"].astype(\"category\")\n",
    "# print(training.label.describe())\n",
    "\n",
    "labels = pd.get_dummies(training['label']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Parameters----#\n",
    "\n",
    "# encodes input sequence dense vectors \n",
    "embed_dim = 128\n",
    "\n",
    "# transforms the vector sequence into a single vector\n",
    "lstm_out = 200\n",
    "\n",
    "# batch size of 32 is a good starting point\n",
    "batch_size = 32\n",
    "\n",
    "# epochs\n",
    "nb_epoch = 10\n",
    "\n",
    "#------# Build the LSTM model #-----------------#\n",
    "\n",
    "print('lets goooo...') \n",
    "\n",
    "# Initialising the RNN\n",
    "model = Sequential()\n",
    "\n",
    "#adding an input layer and the first hidden layer\n",
    "model.add(Embedding(2500, embed_dim, \n",
    "                    input_length = features.shape[1], \n",
    "                    dropout = 0.2)) \n",
    "# Adding the second hidden layer\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "# Adding the output layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile( optimizer='adam', # optimazer\n",
    "              loss = 'categorical_crossentropy', # loss function\n",
    "              metrics = ['accuracy']) # list of metrics\n",
    "\n",
    "model.name = 'LSTM model'\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, Y_train, \n",
    "                    validation_split=0.33, \n",
    "                    batch_size = batch_size, \n",
    "                    nb_epoch = nb_epoch, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
