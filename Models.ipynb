{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.porter import * \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>neg_scores</th>\n",
       "      <th>neu_scores</th>\n",
       "      <th>pos_scores</th>\n",
       "      <th>compound_scores</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>thought sleep option tomorrow realiz evalu mor...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>life cool</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                             tokens  neg_scores  \\\n",
       "0    0.0                                                0.0         0.0   \n",
       "1    0.0  thought sleep option tomorrow realiz evalu mor...         0.0   \n",
       "2    0.0                                          life cool         0.0   \n",
       "\n",
       "   neu_scores  pos_scores  compound_scores  NOUN  PRON  VERB  ADJ  ADV  \n",
       "0       0.000       0.000           0.0000   1.0   0.0   0.0  1.0  0.0  \n",
       "1       1.000       0.000           0.0000   1.0   0.0   1.0  1.0  0.0  \n",
       "2       0.303       0.697           0.3182   1.0   0.0   1.0  0.0  0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "training = pd.read_csv('feat_eng_train_data.csv')\n",
    "\n",
    "# remove rows with none values\n",
    "training = training.dropna(0, 'any')\n",
    "\n",
    "# Features TODO: correct feature names\n",
    "features = ['tokens', 'neu_scores', 'neg_scores', 'compound_scores', 'pos_scores']\n",
    "label = ['score']\n",
    "\n",
    "# Saving features and label data in X and y for train-test split\n",
    "X = training[[col for col in training.columns if col in features]]\n",
    "y = training[label]\n",
    "\n",
    "# splitting data into training and validation set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "training.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from gracecarrillo\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "# Pipeline to convert tweets to a matrix of TF-IDF features.\n",
    "tfidf = Pipeline([\n",
    "                ('selector', TextSelector(key='tokens')),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])\n",
    "\n",
    "# Pipeline to convert tweets to a matrix of token counts\n",
    "countvect = Pipeline([\n",
    "                ('selector', TextSelector(key='tokens')),\n",
    "                ('countvect', CountVectorizer())\n",
    "            ])\n",
    "\n",
    "# Applying tfidf anf countvec to features\n",
    "neu_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='neu_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])\n",
    "neg_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='neg_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])\n",
    "pos_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='pos_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])\n",
    "\n",
    "compound_scores =  Pipeline([\n",
    "                ('selector', NumberSelector(key='compound_scores')),\n",
    "                ('minmax', MinMaxScaler())\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining different sets of text processors\n",
    "def features_union(textProcessor):\n",
    "    return FeatureUnion([('tokens', textProcessor),\n",
    "                      ('neu_scores', neu_scores),\n",
    "                      ('neg_scores', neg_scores),\n",
    "                      ('pos_scores', pos_scores),\n",
    "                      ('compound_scores', compound_scores)])\n",
    "# Normalise labels\n",
    "le = LabelEncoder().fit(y_train.values.ravel()) #if error occurs, try removing or adding values before ravel\n",
    "\n",
    "y_train = le.transform(y_train.values.ravel())\n",
    "y_test = le.transform(y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7382150043115838"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive-Bayes Classifier\n",
    "\n",
    "# instantiate classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# combine features\n",
    "features_count = features_union(countvect)\n",
    "\n",
    "# define pipeline object \n",
    "nb_pipeline = Pipeline([('features', features_count),\n",
    "                       ('nb', clf)])\n",
    "\n",
    "# Fit classifier\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# score\n",
    "nb_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7342627191721759"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "\n",
    "# instantiate classifier\n",
    "svm = LinearSVC()\n",
    "\n",
    "#  combine features\n",
    "features_tfidf = features_union(tfidf)\n",
    "\n",
    "# define pipeline object\n",
    "svm_pipeline = Pipeline([('features', features_tfidf),\n",
    "                       ('svm', svm)])\n",
    "\n",
    "# Fit classifier\n",
    "svm_pipeline.fit(X_train, y_train.ravel())\n",
    "\n",
    "# score\n",
    "svm_pipeline.score(X_test, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 12.1 minutes\n",
      "0.7510060362173038\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# cross valiadation for Naive-Bayes Classifier\n",
    "\n",
    "# instantiate pipeline object\n",
    "nb_pipeline = Pipeline([('feats', features_tfidf),  ('clf', MultinomialNB())])\n",
    "\n",
    "# parameter grid (3x3x2x2x3x3x2) combinations\n",
    "parameters = {\n",
    "    'feats__tokens__tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'feats__tokens__tfidf__ngram_range': ((1, 1), (1, 2), (2, 2)), \n",
    "    'feats__tokens__tfidf__use_idf': (False, True),\n",
    "    'feats__tokens__tfidf__binary':(False, True),\n",
    "    'feats__tokens__tfidf__binary':('l1', 'l2', None),\n",
    "    'clf__alpha': (1.0, 5.0, 10.0),\n",
    "    'clf__fit_prior': (True, False),     \n",
    "}\n",
    "\n",
    "# instantiate GridSearchCV object with pipeline and parameters with 3-folds cross-validation\n",
    "nb_grid = GridSearchCV(nb_pipeline, parameters, cv=3)\n",
    "\n",
    "# start time \n",
    "nb_start = time.time()\n",
    "\n",
    "# Fit \n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "# end time \n",
    "nb_end = time.time()\n",
    "print(f\"Time taken to run: {round((nb_end - nb_start)/60,1)} minutes\")\n",
    "\n",
    "# Check score\n",
    "print(nb_grid.score(X_test, y_test))\n",
    "\n",
    "nb_cv_results = pd.DataFrame(nb_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run: 25.5 minutes\n"
     ]
    }
   ],
   "source": [
    "# cross validation for SVM Classifier\n",
    "\n",
    "# instantiate pipeline\n",
    "svm_count_pipeline = Pipeline([('feats', features_count),  ('clf', LinearSVC())])\n",
    "\n",
    "# parameter grid (3x3x2x3x7x2) combinations\n",
    "parameters = {\n",
    "    'feats__tokens__countvect__max_df': (0.5, 0.75, 1.0),\n",
    "    'feats__tokens__countvect__ngram_range': ((1, 1), (1, 2), (2, 2)), \n",
    "    #'feats__tokens__countvect__use_idf': (False, True),\n",
    "    'clf__loss': ('hinge', 'squared_hinge'),\n",
    "    'clf__C': (0.1, 0.5, 0.6, 1, 4, 5, 10, 100),\n",
    "    'clf__class_weight': (None, 'balanced')                                    \n",
    "}\n",
    "\n",
    "# instantiate GridSearchCV object with pipeline and parameters with 3-folds cross-validation\n",
    "svm_grid = GridSearchCV(svm_count_pipeline, parameters, cv=3)\n",
    "\n",
    "# start time \n",
    "svm_start = time.time()\n",
    "\n",
    "# fit\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# end time \n",
    "svm_end = time.time()\n",
    "print(f\"Time taken to run: {round((svm_end - svm_start)/60,1)} minutes\")\n",
    "\n",
    "# score\n",
    "svm_grid.score(X_test, y_test)\n",
    "\n",
    "svm_cv_results = pd.DataFrame(svm_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter indicating the number of words\n",
    "nb_words = 10000  \n",
    "\n",
    "# create the tokenizer (tweets have been preprocessed so no need for filters)\n",
    "tk = Tokenizer(num_words=nb_words)\n",
    "\n",
    "# fit the tokenizer on tweets\n",
    "tk.fit_on_texts(training.tokens)\n",
    "\n",
    "# integer encode tweets\n",
    "tweets_seq = tk.texts_to_sequences(training.tokens)\n",
    "\n",
    "# TODO need to update based, uncomment line below to see what the max is\n",
    "# print(training['word count'].describe())\n",
    "max_len = 39\n",
    "\n",
    "# Convert sequences into 2-D Numpy arrays\n",
    "features = pad_sequences(tweets_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"label\"] = training[\"label\"].astype(\"category\")\n",
    "# print(training.label.describe())\n",
    "\n",
    "labels = pd.get_dummies(training['label']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Parameters----#\n",
    "\n",
    "# encodes input sequence dense vectors \n",
    "embed_dim = 128\n",
    "\n",
    "# transforms the vector sequence into a single vector\n",
    "lstm_out = 200\n",
    "\n",
    "# batch size of 32 is a good starting point\n",
    "batch_size = 32\n",
    "\n",
    "# epochs\n",
    "nb_epoch = 10\n",
    "\n",
    "#------# Build the LSTM model #-----------------#\n",
    "reg_model = Sequential()\n",
    "reg_model.add(Embedding(2500, embed_dim, input_length = features.shape[1], dropout = 0.2))\n",
    "reg_model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "reg_model.add(Dense(2, kernel_regularizer=regularizers.l2(0.001), activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "reg_model.compile( optimizer='adam', # optimazer\n",
    "              loss = 'categorical_crossentropy', # loss function\n",
    "              metrics = ['accuracy']) # list of metrics\n",
    "\n",
    "reg_model.name = 'LSTM with Regularisation model'\n",
    "print(reg_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "reg_history = reg_model.fit(X_train, Y_train, \n",
    "                    validation_split=0.33, \n",
    "                    batch_size = batch_size, \n",
    "                    nb_epoch = nb_epoch, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update to be the file path that we want\n",
    "\n",
    "# save model and architecture\n",
    "reg_model.save('LSTM_regmodel.h5')\n",
    "\n",
    "#-- LSTM model ----#\n",
    "y_preds_LSTM = model.predict(X_test)\n",
    "\n",
    "# Save predictions for evaluation as numpy arrays\n",
    "np.save('y_predsLSTM.npy', y_preds_LSTM)\n",
    "\n",
    "#-- LSTM with regularisation model ----#\n",
    "y_preds_LSTMreg = reg_model.predict(X_test)\n",
    "\n",
    "# Save predictions for evaluation as numpy arrays\n",
    "np.save('y_predsLSTMreg.npy', y_preds_LSTMreg)\n",
    "\n",
    "# Save test data\n",
    "np.save('y_testLSTM.npy', Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
